{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":39763,"databundleVersionId":11756775,"sourceType":"competition"},{"sourceId":11569755,"sourceType":"datasetVersion","datasetId":7253661},{"sourceId":11752314,"sourceType":"datasetVersion","datasetId":7377931}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":1420.265378,"end_time":"2025-05-09T22:49:47.231153","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-09T22:26:06.965775","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n\nif not torch.cuda.is_available() or torch.cuda.device_count() < 2:\n    raise RuntimeError(\"Requires >= 2 GPUs with CUDA enabled.\")\n\nRUN_TRAIN = True\nRUN_VALID = True\nRUN_TEST  = True","metadata":{"execution":{"iopub.execute_input":"2025-05-09T22:26:11.508753Z","iopub.status.busy":"2025-05-09T22:26:11.508535Z","iopub.status.idle":"2025-05-09T22:26:16.033824Z","shell.execute_reply":"2025-05-09T22:26:16.033001Z"},"papermill":{"duration":4.532562,"end_time":"2025-05-09T22:26:16.03548","exception":false,"start_time":"2025-05-09T22:26:11.502918","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# HGNet-V2 Starter Notebook\n\nThis notebook builds on Egor Trushin's great starter notebook [here](https://www.kaggle.com/code/egortrushin/gwi-unet-with-float16-dataset), thanks for sharing.\n\nThe main purpose of this notebook is to show how to use 2 GPUs during model training, maximizing our weekly GPU quota in the Kaggle environment. \n\nIn addition, I provide 2x pretrained model checkpoints that were trained for 50 epochs using this setup. Each model achieved a validation MAE of ~65-70.\n\nOther additions:\n- Flip augmentation\n- Dataset preprocessing\n- EMA (Exponential moving average)\n- Unet w/ a pretrained encoder","metadata":{"execution":{"iopub.execute_input":"2025-04-25T14:53:18.823481Z","iopub.status.busy":"2025-04-25T14:53:18.823205Z","iopub.status.idle":"2025-04-25T14:53:23.506381Z","shell.execute_reply":"2025-04-25T14:53:23.505626Z","shell.execute_reply.started":"2025-04-25T14:53:18.823463Z"},"papermill":{"duration":0.003598,"end_time":"2025-05-09T22:26:16.043564","exception":false,"start_time":"2025-05-09T22:26:16.039966","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile config.yaml\n\nlocal_rank: 0\ndata_path: \"/kaggle/input/openfwi-preprocessed-72x72/openfwi_72x72/\"\nmodel_path: \"/kaggle/input/openfwi-preprocessed-72x72/models/\"\nbackbone: \"hgnetv2_b4.ssld_stage2_ft_in1k\"\nbatch_size: 256\nprint_freq: 100\nmax_epochs: 1\nes_epochs: 3\nseed: 99\noptimizer:\n    lr: 0.001\n    weight_decay: 0.001\nscheduler:\n    params:\n        factor: 0.8\n        patience: 0","metadata":{"execution":{"iopub.execute_input":"2025-05-09T22:26:16.051796Z","iopub.status.busy":"2025-05-09T22:26:16.051516Z","iopub.status.idle":"2025-05-09T22:26:16.056336Z","shell.execute_reply":"2025-05-09T22:26:16.055735Z"},"papermill":{"duration":0.010185,"end_time":"2025-05-09T22:26:16.057397","exception":false,"start_time":"2025-05-09T22:26:16.047212","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocess\n\nHere we reduce the size of each input to (72,72), and then save as fp16. This reduces the size of each input to roughly 4% of the original size.\n\nThis has already been done for every datapoint in the OpenFWI dataset and can be found [here](https://www.kaggle.com/datasets/brendanartley/openfwi-preprocessed-72x72).","metadata":{"papermill":{"duration":0.003461,"end_time":"2025-05-09T22:26:16.064823","exception":false,"start_time":"2025-05-09T22:26:16.061362","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\ndef _preprocess(x):\n    x = F.interpolate(x, size=(70, 70), mode='area')\n    x = F.pad(x, (1,1,1,1), mode='replicate')\n    return x\n\ndef _helper(x, ):\n    before_shape = x.shape\n    before_mem = x.nbytes / 1e6\n    x = torch.from_numpy(x).float()\n\n    # Interpolate and pad\n    x = _preprocess(x)\n    x = x.cpu().numpy().astype(np.float16)\n\n    after_mem = x.nbytes / 1e6\n    percent = 100 - 100 * (before_mem - after_mem) / before_mem if before_mem else 0\n\n    # Log\n    print(\"Shape Change\")\n    print(\"  {} -> {}\".format(before_shape, x.shape))\n    print()\n    print(\"Memory Usage\")\n    print(\"  {:.1f} MB -> {:.1f} MB\".format(before_mem, after_mem))\n    print(\"  ({:.1f}% of original size)\".format(percent))\n    return x\n","metadata":{"execution":{"iopub.execute_input":"2025-05-09T22:26:16.07332Z","iopub.status.busy":"2025-05-09T22:26:16.07287Z","iopub.status.idle":"2025-05-09T22:26:16.07863Z","shell.execute_reply":"2025-05-09T22:26:16.07799Z"},"papermill":{"duration":0.011067,"end_time":"2025-05-09T22:26:16.079673","exception":false,"start_time":"2025-05-09T22:26:16.068606","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocess\nx= np.load(\"/kaggle/input/waveform-inversion/train_samples/CurveFault_A/seis2_1_0.npy\")\nx = _helper(x)\n\n# Sanity check: Confirm preprocessing matches w/ Dataset\nz= np.load(\"/kaggle/input/openfwi-preprocessed-72x72/openfwi_72x72/CurveFault_A/seis2_1_0.npy\")\nassert np.all(z == x)\n\ndel x, z","metadata":{"execution":{"iopub.execute_input":"2025-05-09T22:26:16.087848Z","iopub.status.busy":"2025-05-09T22:26:16.087653Z","iopub.status.idle":"2025-05-09T22:26:20.261598Z","shell.execute_reply":"2025-05-09T22:26:20.260697Z"},"papermill":{"duration":4.179493,"end_time":"2025-05-09T22:26:20.262866","exception":false,"start_time":"2025-05-09T22:26:16.083373","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataset\n\nHere, we introduce a flip augmentation. \n\nUnlike a normal horizontal flip, we have to reverse the source and receiver dimensions. To match this, we reverse the width dimension of the label as well.\n\nWe use this flip as TTA (test-time augmentation) during inference.","metadata":{"papermill":{"duration":0.003636,"end_time":"2025-05-09T22:26:20.27078","exception":false,"start_time":"2025-05-09T22:26:20.267144","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile data.py\n\nimport glob\nimport numpy as np\nfrom torch.utils.data import Dataset\n\ndef inputs_files_to_output_files(input_files):\n    return [\n        f.replace('/seis', '/vel').replace('/data', '/model')\n        for f in input_files\n    ]\n\n\ndef get_data_files(data_path):\n\n    # All filenames\n    all_inputs = [\n        f for f in glob.glob(data_path + \"/*/*.npy\")\n        if ('/seis' in f) or ('/data' in f)\n    ]\n    all_outputs = inputs_files_to_output_files(all_inputs)\n    assert all([x != y for x,y in zip(all_inputs, all_outputs)])\n\n    # Validation filenames\n    val_fpaths= [\n        'CurveFault_A/seis2_1_0.npy', 'CurveFault_A/seis2_1_1.npy', \n        'CurveFault_B/seis6_1_0.npy', 'CurveFault_B/seis6_1_1.npy', \n        'CurveVel_A/data1.npy', 'CurveVel_A/data10.npy', \n        'CurveVel_B/data1.npy', 'CurveVel_B/data10.npy', \n        'FlatFault_A/seis2_1_0.npy', 'FlatFault_A/seis2_1_1.npy', \n        'FlatFault_B/seis6_1_0.npy', 'FlatFault_B/seis6_1_1.npy', \n        'FlatVel_A/data1.npy', 'FlatVel_A/data10.npy', \n        'FlatVel_B/data1.npy', 'FlatVel_B/data10.npy', \n        'Style_A/data1.npy', 'Style_A/data10.npy', \n        'Style_B/data1.npy', 'Style_B/data10.npy',\n        ]\n\n    train_inputs, train_outputs= [], []\n    valid_inputs, valid_outputs= [], []\n\n    # Iterate and split files\n    for a,b in zip(all_inputs, all_outputs):\n        to_val= False\n        \n        for c in val_fpaths:\n            if c in a:\n                to_val= True\n\n        if to_val:\n            valid_inputs.append(a)\n            valid_outputs.append(b)\n        else:\n            train_inputs.append(a)\n            train_outputs.append(b)\n\n    return train_inputs, train_outputs, valid_inputs, valid_outputs\n\n\n\nclass SeismicDataset(Dataset):\n    def __init__(self, inputs_files, output_files, mode, n_examples_per_file=500):\n        assert len(inputs_files) == len(output_files)\n        self.inputs_files = inputs_files\n        self.output_files = output_files\n        self.n_examples_per_file = n_examples_per_file\n        self.mode= mode\n\n    def __len__(self):\n        return len(self.inputs_files) * self.n_examples_per_file\n\n    def __getitem__(self, idx):\n        # Calculate file offset and sample offset within file\n        file_idx = idx // self.n_examples_per_file\n        sample_idx = idx % self.n_examples_per_file\n\n        x = np.load(self.inputs_files[file_idx], mmap_mode='r')[sample_idx]\n        y = np.load(self.output_files[file_idx], mmap_mode='r')[sample_idx]\n\n        # Random Flips\n        if self.mode == \"train\":\n\n            # Receiver Flip\n            if np.random.random() < 0.5:\n                x= x[::-1, :, ::-1]\n                y= y[:, ::-1]\n            \n        try:\n            return x.copy(), y.copy()\n        finally:\n            del x, y\n\n\nclass TestDataset(Dataset):\n    def __init__(self, test_files):\n        self.test_files = test_files\n\n    def __len__(self):\n        return len(self.test_files)\n\n    def __getitem__(self, i):\n        test_file = self.test_files[i]\n        test_stem = test_file.split(\"/\")[-1].split(\".\")[0]\n        return np.load(test_file), test_stem","metadata":{"execution":{"iopub.execute_input":"2025-05-09T22:26:20.279368Z","iopub.status.busy":"2025-05-09T22:26:20.279144Z","iopub.status.idle":"2025-05-09T22:26:20.284681Z","shell.execute_reply":"2025-05-09T22:26:20.283895Z"},"papermill":{"duration":0.011441,"end_time":"2025-05-09T22:26:20.28583","exception":false,"start_time":"2025-05-09T22:26:20.274389","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Utils\n\nSame as Egor's.","metadata":{"papermill":{"duration":0.003597,"end_time":"2025-05-09T22:26:20.293365","exception":false,"start_time":"2025-05-09T22:26:20.289768","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile utils.py\n\nimport datetime\nimport random\nimport torch\nimport numpy as np\n\ndef format_time(elapsed):\n    \"\"\"Take a time in seconds and return a string hh:mm:ss.\"\"\"\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n\ndef seed_everything(\n    seed_value: int\n) -> None:\n    \"\"\"\n    Controlling a unified seed value for Python, NumPy, and PyTorch (CPU, GPU).\n\n    Parameters:\n    ----------\n    seed_value : int\n        The unified random seed value.\n    \"\"\"\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n    if torch.backends.cudnn.is_available:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.execute_input":"2025-05-09T22:26:20.301939Z","iopub.status.busy":"2025-05-09T22:26:20.301324Z","iopub.status.idle":"2025-05-09T22:26:20.30583Z","shell.execute_reply":"2025-05-09T22:26:20.305313Z"},"papermill":{"duration":0.009828,"end_time":"2025-05-09T22:26:20.30688","exception":false,"start_time":"2025-05-09T22:26:20.297052","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model\n\nThis model includes several modifications beyond a standard U-Net architecture.\n\n\n### Encoder\n\nThe model uses the `HgnetV2` backbone from timm as the encoder. We have to make a few modifications for this to work with the Unet. See more info on this backbone [here](https://huggingface.co/timm/hgnetv2_b4.ssld_stage1_in22k_in1k).\n\n\nFirst, we reduce the stride of the stem convolution from (2,2) to (1,1). This increases the size of the feature maps in the backbone. Second, we reduce the stride of the downsample convolution in the deepest block from (2,2) to (1,1). We do this so that upsampling in the decoder can be done without padding.\n\n```python\n# Original feature map\n[torch.Size([18, 18]), torch.Size([9, 9]), torch.Size([5, 5]), torch.Size([3, 3])]\n\n# Updated stem conv\n[torch.Size([36, 36]), torch.Size([18, 18]), torch.Size([9, 9]), torch.Size([5, 5])]\n\n# Updated downsample conv\n[torch.Size([36, 36]), torch.Size([18, 18]), torch.Size([9, 9]), torch.Size([9, 9])]\n```\n\n### Decoder\n\nThe decoder has a few modifications as well. \n\nWe remove all BatchNorm2d layers and add intermediate convolutions to the skip connections. I found that removing the normalization layers increased the convergence speed, and the intermediate convolutions improved the model's predictiveness.","metadata":{"papermill":{"duration":0.00371,"end_time":"2025-05-09T22:26:20.314443","exception":false,"start_time":"2025-05-09T22:26:20.310733","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile model.py\n\nimport torch\nimport torch.nn as nn\nimport timm\n\nclass EnsembleModel(nn.Module):\n    def __init__(self, models):\n        super().__init__()\n        self.models = nn.ModuleList(models).eval()\n\n    def forward(self, x):\n        output = None\n        \n        for m in self.models:\n            logits= m(x)\n            \n            if output is None:\n                output = logits\n            else:\n                output += logits\n                \n        output /= len(self.models)\n        return output\n\n\nclass ConvBnAct2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        padding: int = 0,\n        stride: int = 1,\n    ):\n        super().__init__()\n\n        self.conv= nn.Conv2d(\n            in_channels, \n            out_channels,\n            kernel_size,\n            stride=stride, \n            padding=padding, \n            bias=False,\n        )\n        self.act= nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.act(x)\n        return x\n\nclass DecoderBlock2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        skip_channels,\n        out_channels,\n        scale_factor: int = 2,\n    ):\n        super().__init__()\n\n        self.upsample = nn.ConvTranspose2d(\n            in_channels= in_channels,\n            out_channels= in_channels,\n            kernel_size=scale_factor, \n            stride=scale_factor,\n        )\n\n        k= 3\n        c= skip_channels if skip_channels != 0 else in_channels\n        self.intermediate_conv = nn.Sequential(\n            ConvBnAct2d(c, c, k, k//2),\n            ConvBnAct2d(c, c, k, k//2),\n            )\n\n        self.conv1 = ConvBnAct2d(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size= 3,\n            padding= 1,\n        )\n\n        self.conv2 = ConvBnAct2d(\n            out_channels,\n            out_channels,\n            kernel_size= 3,\n            padding= 1,\n        )\n\n    def forward(self, x, skip=None):\n        x = self.upsample(x)\n\n        if skip is not None:\n            skip = self.intermediate_conv(skip)\n            x = torch.cat([x, skip], dim=1)\n        else:\n            x = self.intermediate_conv(x)\n\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x\n\n\nclass UnetDecoder2d(nn.Module):\n    \"\"\"\n    Unet decoder.\n    Source: https://arxiv.org/abs/1505.04597\n    \"\"\"\n    def __init__(\n        self,\n        encoder_channels: tuple[int],\n        skip_channels: tuple[int] = None,\n        decoder_channels: tuple = (256, 128, 64, 32, 16),\n        scale_factors: tuple = (1,2,2,2),\n    ):\n        super().__init__()\n        \n        if len(encoder_channels) == 4:\n            decoder_channels= decoder_channels[1:]\n        self.decoder_channels= decoder_channels\n        \n        if skip_channels is None:\n            skip_channels= list(encoder_channels[1:]) + [0]\n\n        # Build decoder blocks\n        in_channels= [encoder_channels[0]] + list(decoder_channels[:-1])\n        self.blocks = nn.ModuleList()\n\n        for i, (ic, sc, dc, sf) in enumerate(zip(\n            in_channels, skip_channels, decoder_channels, scale_factors,\n        )):\n            self.blocks.append(\n                DecoderBlock2d(\n                    ic, sc, dc, \n                    scale_factor= sf,\n                    )\n            )\n\n    def forward(self, feats: list[torch.Tensor]):\n        res= [feats[0]]\n        feats= feats[1:]\n\n        # Decoder blocks\n        for i, b in enumerate(self.blocks):\n            skip= feats[i] if i < len(feats) else None\n            res.append(\n                b(res[-1], skip=skip),\n                )\n            \n        return res\n\nclass SegmentationHead2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        scale_factor: tuple[int] = (2,2),\n        kernel_size: int = 3,\n    ):\n        super().__init__()\n        self.conv= nn.Conv2d(\n            in_channels, out_channels, kernel_size= kernel_size,\n            padding= kernel_size//2\n        )\n        self.upsample = nn.Upsample(\n            scale_factor= scale_factor,\n            mode='bilinear',\n            align_corners= False\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.upsample(x)\n        return x\n\nclass HGUNet(nn.Module):\n    def __init__(\n        self,\n        backbone: str,\n    ):\n        super().__init__()\n\n        # Encoder\n        self.backbone= timm.create_model(\n            backbone,\n            in_chans= 5,\n            pretrained= True,\n            features_only= True,\n            drop_path_rate= 0.2,\n            )\n        ecs= [_[\"num_chs\"] for _ in self.backbone.feature_info][::-1]\n\n        # Decoder\n        self.decoder= UnetDecoder2d(\n            encoder_channels= ecs,\n            decoder_channels= (256, 128, 64, 32),\n            scale_factors= (1,2,2,2),\n        )\n\n        self.seg_head= SegmentationHead2d(\n            in_channels= self.decoder.decoder_channels[-1],\n            out_channels= 1,\n            scale_factor= 2,\n        )\n        self._update_stem()\n\n    def _update_stem(self, ):\n        self.backbone.stem.stem1.conv.stride=(1,1)\n        self.backbone.stages_3.downsample.conv.stride=(1,1)\n        pass\n\n        \n    def proc_flip(self, x_in):\n        # Flip TTA during inference\n        x_in= torch.flip(x_in, dims=[-3, -1])\n        \n        x= self.backbone(x_in)\n        x= x[::-1]\n        x= self.decoder(x)\n        x_seg= self.seg_head(x[-1])\n        x_seg= x_seg[..., 1:-1, 1:-1]\n        x_seg= x_seg * 1500 + 3000\n        \n        x_seg= torch.flip(x_seg, dims=[-1])\n        return x_seg\n\n    def forward(self, x):        \n        x_in = x\n        \n        x= self.backbone(x)\n        x= x[::-1]\n        x= self.decoder(x)\n        x_seg= self.seg_head(x[-1])\n        x_seg= x_seg[..., 1:-1, 1:-1]\n        x_seg= x_seg * 1500 + 3000\n    \n        if self.training:\n            return x_seg\n        else:\n            p1 = self.proc_flip(x_in)\n            x_seg = torch.mean(torch.stack([x_seg, p1]), dim=0)\n            return x_seg","metadata":{"execution":{"iopub.execute_input":"2025-05-09T22:26:20.323097Z","iopub.status.busy":"2025-05-09T22:26:20.322875Z","iopub.status.idle":"2025-05-09T22:26:20.328981Z","shell.execute_reply":"2025-05-09T22:26:20.328471Z"},"papermill":{"duration":0.011679,"end_time":"2025-05-09T22:26:20.330017","exception":false,"start_time":"2025-05-09T22:26:20.318338","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### EMA\n\nThis is a common strategy used to increase the stability of validation performance between steps/epochs. This implementation is from Tereka [here](https://www.kaggle.com/competitions/blood-vessel-segmentation/discussion/475080#2641635).\n\nNote: We have to be patient with EMA. We should see good results after 5 epochs or so.","metadata":{"papermill":{"duration":0.003684,"end_time":"2025-05-09T22:26:20.337538","exception":false,"start_time":"2025-05-09T22:26:20.333854","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile ema.py\n\nfrom copy import deepcopy\nimport torch\nimport torch.nn as nn\n\nclass ModelEMA(nn.Module):\n    def __init__(self, model, decay=0.99, device=None):\n        super().__init__()\n        self.module = deepcopy(model)\n        self.module.eval()\n        self.decay = decay\n        self.device = device\n        if self.device is not None:\n            self.module.to(device=device)\n\n    def _update(self, model, update_fn):\n        with torch.no_grad():\n            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n                if self.device is not None:\n                    model_v = model_v.to(device=self.device)\n                ema_v.copy_(update_fn(ema_v, model_v))\n\n    def update(self, model):\n        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n\n    def set(self, model):\n        self._update(model, update_fn=lambda e, m: m)","metadata":{"execution":{"iopub.execute_input":"2025-05-09T22:26:20.346044Z","iopub.status.busy":"2025-05-09T22:26:20.345624Z","iopub.status.idle":"2025-05-09T22:26:20.349791Z","shell.execute_reply":"2025-05-09T22:26:20.349287Z"},"papermill":{"duration":0.009408,"end_time":"2025-05-09T22:26:20.350778","exception":false,"start_time":"2025-05-09T22:26:20.34137","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train\n\nHere is the main training script. \n\nBy using 2x GPUs, we can use larger batch sizes and speed up model training. No more wasted Quota!\n\nI won't go into the details of the script as there are already many good resources explaining DDP. Here are a couple of good starting points. \n\n- [Run DDP scripts with 2 T 4](https://www.kaggle.com/code/cpmpml/run-ddp-scripts-with-2-t-4) by @CPMP\n- [Getting Started with Distributed Data Parallel\n](https://docs.pytorch.org/tutorials/intermediate/ddp_tutorial.html)\n- [Distributed Data Parallel Docs](https://docs.pytorch.org/docs/stable/notes/ddp.html)","metadata":{"papermill":{"duration":0.003652,"end_time":"2025-05-09T22:26:20.358275","exception":false,"start_time":"2025-05-09T22:26:20.354623","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%writefile train.py\n\nimport os\nimport yaml\nimport time\nimport gc\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\n\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.utils.data import DataLoader, DistributedSampler\n\nfrom data import inputs_files_to_output_files, get_data_files, SeismicDataset\nfrom utils import format_time, seed_everything\nfrom model import HGUNet\nfrom ema import ModelEMA\n\ndef printy(*args):\n    try: \n        global cfg\n        assert cfg[\"local_rank\"] == 0\n    except:\n        print(*args)\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef setup(rank, world_size):\n    torch.cuda.set_device(rank)\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    return\n\ndef cleanup():\n    dist.barrier()\n    dist.destroy_process_group()\n    return\n\ndef train(cfg):\n\n    # ========= Datasets / Dataloaders ==========\n    printy(\"=\"*25)\n    train_inputs, train_outputs, valid_inputs, valid_outputs = get_data_files(cfg[\"data_path\"])\n    printy(\"TRAIN_FILES:\", len(train_inputs))\n    printy(\"VALID_FILES:\", len(valid_inputs))\n    printy(\"=\"*25)\n\n    dstrain = SeismicDataset(train_inputs, train_outputs, mode=\"train\")\n    samplertrain= DistributedSampler(\n        dstrain, \n        num_replicas=cfg[\"world_size\"], \n        rank=cfg[\"local_rank\"],\n        )\n    dltrain = DataLoader(\n        dstrain,\n        sampler= samplertrain,\n        batch_size=cfg[\"batch_size\"],\n        pin_memory=False,\n        drop_last=True,\n        num_workers=4,\n        persistent_workers=False,\n    )\n\n    dsvalid = SeismicDataset(valid_inputs, valid_outputs, mode=\"valid\")\n    samplervalid= DistributedSampler(\n        dsvalid, \n        num_replicas=cfg[\"world_size\"], \n        rank=cfg[\"local_rank\"],\n        )\n    dlvalid = DataLoader(\n        dsvalid,\n        sampler= samplervalid,\n        batch_size=32,\n        pin_memory=False,\n        drop_last=False,\n        num_workers=4,\n        persistent_workers=False,\n    )\n    \n\n    # ========== Model / EMA ==========\n    printy(\"=\"*25)\n    model = HGUNet(backbone=cfg[\"backbone\"])\n    model = model.to(cfg[\"local_rank\"])\n    model = DistributedDataParallel(\n        model, \n        device_ids=[cfg[\"local_rank\"]], \n        )\n    n_params= count_parameters(model)\n    printy(\"backbone: {}\".format(cfg[\"backbone\"]))\n    printy(\"n_params: {:_}\".format(count_parameters(model)))\n    \n    ema_model= ModelEMA(model.module, decay=0.99)\n    printy(\"=\"*25)\n\n    \n    # ========== Training ==========\n    criterion = nn.L1Loss()\n    optimizer = torch.optim.AdamW(model.parameters(), **cfg[\"optimizer\"])  # hparams\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', **cfg[\"scheduler\"][\"params\"])\n\n    best_val_loss = 10000.0\n    epochs_wo_improvement = 0\n    t0 = time.time()\n    \n\n    for epoch in range(1, cfg[\"max_epochs\"] + 1):\n\n        # Shuffle sampler\n        dltrain.sampler.set_epoch(epoch)\n    \n        # Train\n        model.train()\n        train_losses = []\n        for step, (inputs, targets) in enumerate(dltrain):\n    \n            inputs = inputs.to(cfg[\"local_rank\"])\n            targets = targets.to(cfg[\"local_rank\"])\n            optimizer.zero_grad()\n    \n            with torch.autocast(device_type=\"cuda\"):\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n    \n            loss.backward()\n            optimizer.step()\n            train_losses.append(loss.item())\n\n            if ema_model is not None:\n                ema_model.update(model.module)\n\n            if cfg[\"local_rank\"] == 0:\n                if step % cfg[\"print_freq\"] == cfg[\"print_freq\"] - 1 or step == len(dltrain) - 1:\n                    trn_loss = np.mean(train_losses)\n                    t1 = format_time(time.time() - t0)\n                    free, total = torch.cuda.mem_get_info(device=0)\n                    mem_used = (total - free) / 1024**3\n                    lr = optimizer.param_groups[-1]['lr']\n                    print(\n                        f\"Epoch: {epoch:02d}  Step {step+1}/{len(dltrain)}  Trn Loss: {trn_loss:.2f}  LR: {lr:.2e}  GPU Usage: {mem_used:.2f}GB  Elapsed Time: {t1}\",\n                        flush=True,\n                    )\n\n\n        # Valid\n        model.eval()\n        valid_losses = []\n        for inputs, targets in dlvalid:\n            inputs = inputs.to(cfg[\"local_rank\"])\n            targets = targets.to(cfg[\"local_rank\"])\n    \n            with torch.inference_mode():\n                with torch.autocast(device_type=\"cuda\"):\n                    \n                    if ema_model is not None:\n                        outputs = ema_model.module(inputs)\n                    else:\n                        outputs = model(inputs)\n    \n            loss = criterion(outputs, targets)\n            valid_losses.append(loss.item())\n\n        # Gather loss on same device\n        v = torch.tensor([sum(valid_losses), len(valid_losses)], device=cfg[\"local_rank\"])\n        torch.distributed.all_reduce(v, op=dist.ReduceOp.SUM)\n        val_loss = (v[0] / v[1]).item()\n\n        scheduler.step(val_loss)\n\n        # Log\n        if cfg[\"local_rank\"] == 0:\n            free, total = torch.cuda.mem_get_info(device=0)\n            mem_used = (total - free) / 1024**3\n            print(\n                f\"\\nEpoch: {epoch:02d}  Trn Loss: {trn_loss:.2f}  Val Loss: {val_loss:.2f}  GPU Usage: {mem_used:.2f}GB  Elapsed Time: {t1}\",\n                flush=True,\n            )\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                epochs_wo_improvement = 0\n                torch.save(ema_model.module.state_dict(), \"best_model.pth\")\n                print(f\"\\nNew best val_loss: {val_loss:.2f}\\n\", flush=True)\n            else:\n                epochs_wo_improvement += 1\n                print(f\"\\nEpochs without improvement: {epochs_wo_improvement}\\n\", flush=True)\n        \n            if epochs_wo_improvement == cfg[\"es_epochs\"]:\n                break\n\n    # Cleanup\n    del model, ema_model, optimizer, scheduler\n    del dltrain, dlvalid, dstrain, dsvalid\n    gc.collect()\n    torch.cuda.empty_cache()\n                            \n    return\n\nif __name__ == \"__main__\":\n\n    # GPU Specs\n    rank = int(os.environ[\"RANK\"])\n    world_size = int(os.environ[\"WORLD_SIZE\"])\n    _, total = torch.cuda.mem_get_info(device=rank)\n\n    # Init\n    setup(rank, world_size)\n    time.sleep(rank)\n    print(f\"Rank: {rank}, World size: {world_size}, GPU memory: {total / 1024**3:.2f}GB\", flush=True)\n    time.sleep(world_size - rank)\n\n    # Load cfg\n    with open(\"config.yaml\", \"r\") as file_obj:\n        cfg = yaml.safe_load(file_obj)\n    cfg[\"local_rank\"]= rank\n    cfg[\"world_size\"]= world_size\n    seed_everything(cfg[\"seed\"]+rank)\n\n    # Run\n    train(cfg)\n    cleanup()","metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2025-05-09T22:26:20.366605Z","iopub.status.busy":"2025-05-09T22:26:20.366428Z","iopub.status.idle":"2025-05-09T22:26:20.3723Z","shell.execute_reply":"2025-05-09T22:26:20.371732Z"},"papermill":{"duration":0.01124,"end_time":"2025-05-09T22:26:20.373257","exception":false,"start_time":"2025-05-09T22:26:20.362017","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if RUN_TRAIN:\n    print(\"Starting training..\")\n    !torchrun --nproc_per_node=2 train.py","metadata":{"execution":{"iopub.execute_input":"2025-05-09T22:26:20.382657Z","iopub.status.busy":"2025-05-09T22:26:20.38226Z","iopub.status.idle":"2025-05-09T22:40:56.934035Z","shell.execute_reply":"2025-05-09T22:40:56.933285Z"},"papermill":{"duration":876.558885,"end_time":"2025-05-09T22:40:56.936537","exception":false,"start_time":"2025-05-09T22:26:20.377652","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Valid\n\nFirst, we load in a 3x pretrained models for inference. These models were trained with with an effective batch_size of 512 (256 per GPU).\n\nThe output prediction is an average of each model.","metadata":{"papermill":{"duration":0.008433,"end_time":"2025-05-09T22:40:56.950735","exception":false,"start_time":"2025-05-09T22:40:56.942302","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import yaml\nimport glob\n\nimport torch\n\nfrom model import HGUNet, EnsembleModel\n\n# Load cfg\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nwith open(\"config.yaml\", \"r\") as file_obj:\n    cfg = yaml.safe_load(file_obj)\n\n# Load models\nfpaths= glob.glob(cfg[\"model_path\"] + \"*.pt\")\nmodels = []\nfor f in fpaths:\n    m = HGUNet(backbone=cfg[\"backbone\"])\n    m.load_state_dict(torch.load(f, map_location=device, weights_only=True))\n    models.append(m)\n\n# Combine models\nmodel = EnsembleModel(models)\nmodel = model.to(device)\nmodel = model.eval()\nprint(\"n_models: {:_}\".format(len(models)))","metadata":{"execution":{"iopub.execute_input":"2025-05-09T22:40:56.964245Z","iopub.status.busy":"2025-05-09T22:40:56.963997Z","iopub.status.idle":"2025-05-09T22:41:05.166064Z","shell.execute_reply":"2025-05-09T22:41:05.165272Z"},"papermill":{"duration":8.211388,"end_time":"2025-05-09T22:41:05.167414","exception":false,"start_time":"2025-05-09T22:40:56.956026","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we score the ensemble on the validation set.","metadata":{"papermill":{"duration":0.0051,"end_time":"2025-05-09T22:41:05.17844","exception":false,"start_time":"2025-05-09T22:41:05.17334","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import gc\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, SequentialSampler\n\nfrom data import inputs_files_to_output_files, get_data_files, SeismicDataset\n\n\nif RUN_VALID:\n\n    # ========= Datasets / Dataloaders ==========\n    print(\"=\"*25)\n    _, _, valid_inputs, valid_outputs = get_data_files(cfg[\"data_path\"])\n    print(\"VALID_FILES:\", len(valid_inputs))\n    print(\"=\"*25)\n    \n    dsvalid = SeismicDataset(valid_inputs, valid_outputs, mode=\"valid\")\n    dlvalid = DataLoader(\n        dsvalid,\n        sampler= SequentialSampler(dsvalid),\n        batch_size=32,\n        num_workers=4,\n    )\n    \n    # ========== Validate ==========\n    criterion = nn.L1Loss()\n    val_logits= []\n    val_targets= []\n    \n    with torch.inference_mode():\n        with torch.autocast(device.type):\n    \n            # Iterate all samples\n            for inputs, targets in tqdm(dlvalid, total=len(dlvalid)):\n                inputs = inputs.to(device)\n                targets = targets.to(device)\n            \n                outputs = model(inputs)\n                \n                val_logits.append(outputs.cpu())\n                val_targets.append(targets.cpu())\n    \n            # Compute Loss\n            val_logits= torch.cat(val_logits, dim=0).float()\n            val_targets= torch.cat(val_targets, dim=0)\n    \n            loss = criterion(\n                input=val_logits,\n                target=val_targets,\n            ).item()\n    \n    print(\"=\"*25)\n    print(\"val_loss: {:.2f}\".format(loss))\n    print(\"=\"*25)\n\n\n    # By class scores\n    print(\"=\"*25)\n    ds_idxs= np.array([_.split(\"/\")[-2] for _ in valid_inputs])\n    ds_idxs= np.repeat(ds_idxs, repeats=500)\n    df= []\n    \n    with torch.no_grad():    \n        for idx in sorted(np.unique(ds_idxs)):\n    \n            # Mask\n            mask = ds_idxs == idx\n            logits_ds = val_logits[mask]\n            targets_ds = val_targets[mask]\n    \n            # Score predictions\n            mae = F.l1_loss(val_logits[mask], val_targets[mask], reduction='mean').item()\n            print(\"{:15} {:.2f}\".format(idx, mae))\n    print(\"=\"*25)\n    \n    # Cleanup\n    del dsvalid, dlvalid\n    del inputs, targets, outputs\n    del ds_idxs, val_logits, val_targets, df\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.execute_input":"2025-05-09T22:41:05.189732Z","iopub.status.busy":"2025-05-09T22:41:05.189484Z","iopub.status.idle":"2025-05-09T22:41:48.971472Z","shell.execute_reply":"2025-05-09T22:41:48.970643Z"},"papermill":{"duration":43.789452,"end_time":"2025-05-09T22:41:48.972931","exception":false,"start_time":"2025-05-09T22:41:05.183479","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test\n\nFinally, we make predictions on the test data.","metadata":{"papermill":{"duration":0.016323,"end_time":"2025-05-09T22:41:49.007116","exception":false,"start_time":"2025-05-09T22:41:48.990793","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import csv\nimport time\n\nfrom data import TestDataset\nfrom utils import format_time\n\nif RUN_TEST:\n    t0 = time.time()\n    \n    test_files = glob.glob(\"/kaggle/input/open-wfi-test/test/*.npy\")\n    x_cols = [f\"x_{i}\" for i in range(1, 70, 2)]\n    fieldnames = [\"oid_ypos\"] + x_cols\n    \n    ds = TestDataset(test_files)\n    dl = DataLoader(ds, batch_size=32, num_workers=4, pin_memory=False)\n    \n    with open(\"submission.csv\", \"wt\", newline=\"\") as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        with torch.inference_mode():\n            with torch.autocast(device.type):\n                for inputs, oids_test in tqdm(dl, total=len(dl)):\n                    inputs = inputs.to(device)\n            \n                    inputs = _preprocess(inputs)\n                    outputs = model(inputs)\n                            \n                    y_preds = outputs[:, 0].cpu().numpy()\n                    \n                    for y_pred, oid_test in zip(y_preds, oids_test):\n                        for y_pos in range(70):\n                            row = dict(zip(x_cols, [y_pred[y_pos, x_pos] for x_pos in range(1, 70, 2)]))\n                            row[\"oid_ypos\"] = f\"{oid_test}_y_{y_pos}\"\n            \n                            writer.writerow(row)\n    \n    t1 = format_time(time.time() - t0)\n    print(f\"Inference Time: {t1}\")","metadata":{"execution":{"iopub.execute_input":"2025-05-09T22:41:49.041752Z","iopub.status.busy":"2025-05-09T22:41:49.041512Z","iopub.status.idle":"2025-05-09T22:49:41.882389Z","shell.execute_reply":"2025-05-09T22:49:41.881116Z"},"papermill":{"duration":472.862217,"end_time":"2025-05-09T22:49:41.886128","exception":false,"start_time":"2025-05-09T22:41:49.023911","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can also view a few samples to make sure things look reasonable.","metadata":{"papermill":{"duration":0.097028,"end_time":"2025-05-09T22:49:42.087013","exception":false,"start_time":"2025-05-09T22:49:41.989985","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import matplotlib.pyplot as plt \n\nif RUN_TEST:\n    # Plot a few samples\n    fig, axes = plt.subplots(3, 5, figsize=(10, 6))\n    axes= axes.flatten()\n\n    n = min(len(outputs), len(axes))\n    \n    for i in range(n):\n        img= outputs[0, 0, ...].cpu().numpy()\n        img = outputs[i, 0].cpu().numpy()\n        idx= oids_test[i]\n    \n        # Plot\n        axes[i].imshow(img, cmap='gray')\n        axes[i].set_title(idx)\n        axes[i].axis('off')\n\n    for i in range(n, len(axes)):\n        axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2025-05-09T22:49:42.27991Z","iopub.status.busy":"2025-05-09T22:49:42.279646Z","iopub.status.idle":"2025-05-09T22:49:43.5402Z","shell.execute_reply":"2025-05-09T22:49:43.539512Z"},"papermill":{"duration":1.362448,"end_time":"2025-05-09T22:49:43.543508","exception":false,"start_time":"2025-05-09T22:49:42.18106","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}